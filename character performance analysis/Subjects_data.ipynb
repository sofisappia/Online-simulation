{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from os import path\n",
    "\n",
    "import wyrm.processing as proc\n",
    "from wyrm import io\n",
    "from wyrm.types import Data, BlockBuffer, RingBuffer\n",
    "from pandas import read_csv      \n",
    "\n",
    "#Classifiers\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay the experiment in real time?\n",
    "REALTIME = False\n",
    "\n",
    "\n",
    "TRAIN_DATA_A = '../../BCI_Comp_III_Wads_2004/data/Subject_A_Train.mat'\n",
    "TEST_DATA_A = '../../BCI_Comp_III_Wads_2004/data/Subject_A_Test.mat'\n",
    "\n",
    "TRAIN_DATA_B = '../../BCI_Comp_III_Wads_2004/data/Subject_B_Train.mat'\n",
    "TEST_DATA_B = '../../BCI_Comp_III_Wads_2004/data/Subject_B_Test.mat'\n",
    "\n",
    "\n",
    "CHANNEL_DATA = '../../BCI_Comp_III_Wads_2004/data/eloc64.txt'\n",
    "\n",
    "TRUE_LABELS_TEST = \"WQXPLZCOMRKO97YFZDEZ1DPI9NNVGRQDJCUVRMEUOOOJD2UFYPOO6J7LDGYEGOA5VHNEHBTXOO1TDOILUEE5BFAEEXAW_K4R3MRU\"\n",
    "\n",
    "STIMULUS_CODE = {\n",
    "    # cols from left to right\n",
    "    1 : \"agmsy5\",\n",
    "    2 : \"bhntz6\",\n",
    "    3 : \"ciou17\",\n",
    "    4 : \"djpv28\",\n",
    "    5 : \"ekqw39\",\n",
    "    6 : \"flrx4_\",\n",
    "    # rows from top to bottom\n",
    "    7 : \"abcdef\",\n",
    "    8 : \"ghijkl\",\n",
    "    9 : \"mnopqr\",\n",
    "    10: \"stuvwx\",\n",
    "    11: \"yz1234\",\n",
    "    12: \"56789_\"\n",
    "}\n",
    "\n",
    "MARKER_DEF_TRAIN = {'target': ['target'], 'nontarget': ['nontarget']}\n",
    "MARKER_DEF_TEST = {i : [i] for i in STIMULUS_CODE.values()}\n",
    "\n",
    "SEG_IVAL = [0, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bci_data(filename, ch_ival):\n",
    "    \"\"\"Load the BCI Competition III Data Set 2.\n",
    "    This method loads the data set and converts it into Wyrm's ``Data``\n",
    "    format. Before you use it, you have to download the data set in\n",
    "    Matlab format and unpack it. The directory with the extracted files\n",
    "    must contain the ``Subject_*.mat``- and the ``eloc64.txt`` files.\n",
    "    .. note::\n",
    "        If you need the true labels of the test sets, you'll have to\n",
    "        download them separately from\n",
    "        http://bbci.de/competition/iii/results/index.html#labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The path to the matlab file to load\n",
    "    Returns\n",
    "    -------\n",
    "    cnt : continuous `Data` object\n",
    "    Examples\n",
    "    --------\n",
    "    >>> dat = load_bcicomp3_ds2('/home/foo/data/Subject_A_Train.mat')\n",
    "    \"\"\"\n",
    "    STIMULUS_CODE = {\n",
    "        0 : \"blankMatrix\",\n",
    "        # cols from left to right\n",
    "        1 : \"agmsy5\",\n",
    "        2 : \"bhntz6\",\n",
    "        3 : \"ciou17\",\n",
    "        4 : \"djpv28\",\n",
    "        5 : \"ekqw39\",\n",
    "        6 : \"flrx4_\",\n",
    "        # rows from top to bottom\n",
    "        7 : \"abcdef\",\n",
    "        8 : \"ghijkl\",\n",
    "        9 : \"mnopqr\",\n",
    "        10: \"stuvwx\",\n",
    "        11: \"yz1234\",\n",
    "        12: \"56789_\"\n",
    "        }\n",
    "\n",
    "    # load the matlab data\n",
    "    data_mat = loadmat(filename)\n",
    "    # load the channel names (the same for all datasets\n",
    "    eloc_file = path.sep.join([path.dirname(filename), 'eloc64.txt'])\n",
    "    with open(eloc_file) as fh:\n",
    "        data = fh.read()\n",
    "    channels = []\n",
    "    for line in data.splitlines():\n",
    "        if line:\n",
    "            chan = line.split()[-1]\n",
    "            chan = chan.replace('.', '')\n",
    "            channels.append(chan)\n",
    "    # fix the channel names, some letters have the wrong capitalization\n",
    "    for i, s in enumerate(channels):\n",
    "        s2 = s.upper()\n",
    "        s2 = s2.replace('Z', 'z')\n",
    "        s2 = s2.replace('FP', 'Fp')\n",
    "        channels[i] = s2\n",
    "    # The signal is recorded with 64 channels, bandpass filtered\n",
    "    # 0.1-60Hz and digitized at 240Hz. The format is Character Epoch x\n",
    "    # Samples x Channels\n",
    "    data = data_mat['Signal'][ch_ival[0]:ch_ival[1],:,:]\n",
    "    data = data.astype('double')\n",
    "#     print('data: ',data.shape)\n",
    "    # For each sample: 1 if a row/colum was flashed, 0 otherwise\n",
    "    flashing = data_mat['Flashing'][ch_ival[0]:ch_ival[1],:]\n",
    "    flashing = flashing.reshape(-1)\n",
    "#     print('flashing: ',flashing.shape)\n",
    "    #flashing = np.flatnonzero((np.diff(a) == 1)) + 1\n",
    "    ##Creates an array where only the initial intensifications of each series appear\n",
    "    tmp = []\n",
    "    for i, _ in enumerate(flashing):\n",
    "        if i == 0:\n",
    "            tmp.append(flashing[i])\n",
    "            continue\n",
    "        if flashing[i] == flashing[i-1] == 1:\n",
    "            tmp.append(0)\n",
    "            continue\n",
    "        tmp.append(flashing[i])\n",
    "    flashing = np.array(tmp)\n",
    "    # For each sample: 0 when no row/colum was intensified,\n",
    "    # 1..6 for intensified columns, 7..12 for intensified rows\n",
    "    stimulus_code = data_mat['StimulusCode'][ch_ival[0]:ch_ival[1],:].reshape(-1)\n",
    "#     print('stim_code: ', stimulus_code.shape)\n",
    "    stimulus_code = stimulus_code[flashing == 1]\n",
    "    # 0 if no row/col was intensified or the intensified did not contain\n",
    "    # the target character, 1 otherwise\n",
    "\n",
    "\n",
    "    # The target characters\n",
    "    target_chars = data_m.get('TargetChar')[0][ch_ival[0]:ch_ival[1]]\n",
    "#     target_chars = data_mat.get('TargetChar', np.array([])).reshape(-1)\n",
    "    \n",
    "    fs = 240\n",
    "    data = data.reshape(-1, 64)\n",
    "    timeaxis = np.linspace(0, data.shape[0] / fs * 1000, data.shape[0], endpoint=False)\n",
    "    dat = Data(data=data, axes=[timeaxis, channels], names=['time', 'channel'], units=['ms', '#'])\n",
    "    dat.fs = fs\n",
    "\n",
    "    #stimulus_code = zip([t for t, _ in flashing], [STIMULUS_CODE[i] for i in stimulus_code])\n",
    "    #Raises error \"TypeError: '<' not supported between instances of 'tuple' and 'list'\" when calling sort() \n",
    "    #stimulus_code =[[t for t,_ in flashing], [STIMULUS_CODE[i] for i in stimulus_code]]\n",
    "    #print(type(stimulus_code), type(flashing), type(targets), type(nontargets))\n",
    "\n",
    "    try:\n",
    "        stimulus_type = data_mat['StimulusType'][ch_ival[0]:ch_ival[1],:].reshape(-1)\n",
    "        target_mask = np.logical_and((flashing == 1), (stimulus_type == 1)) if len(stimulus_type) > 0 else []\n",
    "        nontarget_mask = np.logical_and((flashing == 1), (stimulus_type == 0)) if len(stimulus_type) > 0 else []\n",
    "        targets = [[i, 'target'] for i in timeaxis[target_mask]]\n",
    "        nontargets = [[i, 'nontarget'] for i in timeaxis[nontarget_mask]]\n",
    "    except KeyError:\n",
    "        targets = []\n",
    "        nontargets = []\n",
    "        pass\n",
    "   \n",
    "    # preparing the markers\n",
    "    dat.stimulus_code = stimulus_code[:]\n",
    "    stim = []\n",
    "    flashing = (flashing == 1)\n",
    "    flashing = [[i, 'flashing'] for i in timeaxis[flashing]]\n",
    "    for i,_ in enumerate(flashing):\n",
    "        stim.append([flashing[i][0], STIMULUS_CODE[stimulus_code[i]]])\n",
    "    stimulus_code = stim\n",
    "\n",
    "\n",
    "    markers = flashing[:]\n",
    "    markers.extend(targets)\n",
    "    markers.extend(nontargets)\n",
    "    markers.extend(stimulus_code)\n",
    "    markers.sort()\n",
    "    dat.markers = markers[:]\n",
    "\n",
    "    return dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'B'\n",
    "if subject == 'A':\n",
    "    TRAIN_DATA = TRAIN_DATA_A\n",
    "    TEST_DATA = TEST_DATA_A\n",
    "elif subject == 'B':\n",
    "    TRAIN_DATA = TRAIN_DATA_B\n",
    "    TEST_DATA = TEST_DATA_B  \n",
    "\n",
    "data_m = loadmat(TRAIN_DATA)\n",
    "target_chars = data_m.get('TargetChar')[0][60:]\n",
    "TRUE_LABELS = target_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_A = load_bci_data(TRAIN_DATA_A, ch_ival=[0,42])\n",
    "train_B = load_bci_data(TRAIN_DATA_B, ch_ival=[0,42])\n",
    "train_Abis = load_bci_data(TRAIN_DATA_A, ch_ival=[42,84])\n",
    "train_Bbis = load_bci_data(TRAIN_DATA_B, ch_ival=[42,84])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_A = load_bci_data(TEST_DATA_A, ch_ival=[0,31])\n",
    "test_B = load_bci_data(TEST_DATA_B, ch_ival=[0,31])\n",
    "test_Abis = load_bci_data(TEST_DATA_A, ch_ival=[31,62])\n",
    "test_Bbis = load_bci_data(TEST_DATA_B, ch_ival=[31,62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "labels = TRUE_LABELS_TEST[0:31]\n",
    "labels_bis = TRUE_LABELS_TEST[31:62]\n",
    "train_A.labels = labels\n",
    "train_B.labels = labels\n",
    "train_Abis.labels = labels_bis\n",
    "train_Bbis.labels = labels_bis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sujeto C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bci_data2(filenames, ch_ival, train=True):\n",
    "\n",
    "    STIMULUS_CODE = {\n",
    "            0 : \"blankMatrix\",\n",
    "            # cols from left to right\n",
    "            1 : \"agmsy5\",\n",
    "            2 : \"bhntz6\",\n",
    "            3 : \"ciou17\",\n",
    "            4 : \"djpv28\",\n",
    "            5 : \"ekqw39\",\n",
    "            6 : \"flrx4_\",\n",
    "            # rows from top to bottom\n",
    "            7 : \"abcdef\",\n",
    "            8 : \"ghijkl\",\n",
    "            9 : \"mnopqr\",\n",
    "            10: \"stuvwx\",\n",
    "            11: \"yz1234\",\n",
    "            12: \"56789_\"\n",
    "            }\n",
    "    if train:\n",
    "        n_ch = 42\n",
    "        n_s = 12 #number of samples to erase\n",
    "    else:\n",
    "        n_ch = 31\n",
    "        n_s = 3 #number of samples to erase\n",
    "\n",
    "    # # load the channel names (the same for all datasets\n",
    "    eloc_file = path.sep.join([path.dirname(filenames[0]), 'eloc64.txt'])\n",
    "    with open(eloc_file) as fh:\n",
    "        data = fh.read()\n",
    "    channels = []\n",
    "    for line in data.splitlines():\n",
    "        if line:\n",
    "            chan = line.split()[-1]\n",
    "            chan = chan.replace('.', '')\n",
    "            channels.append(chan)\n",
    "    # # fix the channel names, some letters have the wrong capitalization\n",
    "    for i, s in enumerate(channels):\n",
    "        s2 = s.upper()\n",
    "        s2 = s2.replace('Z', 'z')\n",
    "        s2 = s2.replace('FP', 'Fp')\n",
    "        channels[i] = s2\n",
    "\n",
    "    data_mat0 = loadmat(filenames[0])    \n",
    "    data = data_mat0['signal']\n",
    "    flashing = data_mat0['Flashing'].reshape(-1)\n",
    "    stimulus_code = data_mat0['StimulusCode'].reshape(-1)\n",
    "    stimulus_type = data_mat0.get('StimulusType', np.array([])).reshape(-1)\n",
    "    sequence_phase = data_mat0['PhaseInSequence']\n",
    "\n",
    "    #print(data.shape, flashing.shape, stimulus_code.shape, stimulus_type.shape)\n",
    "\n",
    "    for i, filename in enumerate(filenames):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        # load the matlab data\n",
    "        data_mat = loadmat(filename)\n",
    "        # The signal is recorded with 64 channels, bandpass filtered\n",
    "        # 0.1-60Hz and digitized at 240Hz. The format is Character Epoch x\n",
    "        # Samples x Channels\n",
    "        data_temp = data_mat['signal']\n",
    "        data = np.append(data, data_temp).reshape(-1, 64)\n",
    "        data = data.astype('double')\n",
    "\n",
    "        # For each sample: 1 if a row/colum was flashed, 0 otherwise\n",
    "        flashing = np.append(flashing, data_mat['Flashing'].reshape(-1))\n",
    "        #flashing = np.flatnonzero((np.diff(a) == 1)) + 1\n",
    "        ##Creates an array where only the initial intensifications of each series appear\n",
    "        tmp = []\n",
    "        for i, _ in enumerate(flashing):\n",
    "            if i == 0:\n",
    "                tmp.append(flashing[i])\n",
    "                continue\n",
    "            if flashing[i] == flashing[i-1] == 1:\n",
    "                tmp.append(0)\n",
    "                continue\n",
    "            tmp.append(flashing[i])\n",
    "        flashing = np.array(tmp)\n",
    "\n",
    "        # For each sample: 0 when no row/colum was intensified,\n",
    "        # 1..6 for intensified columns, 7..12 for intensified rows\n",
    "        stimulus_code = np.append(stimulus_code, data_mat['StimulusCode'].reshape(-1))\n",
    "\n",
    "        # 0 if no row/col was intensified or the intensified did not contain\n",
    "        # the target character, 1 otherwise\n",
    "        stimulus_type = np.append(stimulus_type, data_mat.get('StimulusType', np.array([]).reshape(-1)))\n",
    "        sequence_phase = np.append(sequence_phase, data_mat['PhaseInSequence'])\n",
    " \n",
    "    # Erase last 12 samples in order to reshape to (n_ch,-1)\n",
    "    flashing = flashing.reshape(-1)[:-n_s].reshape(n_ch,-1)[ch_ival[0]:ch_ival[1],:].reshape(-1)\n",
    "    stimulus_code = stimulus_code.reshape(-1)[:-n_s].reshape(n_ch,-1)[ch_ival[0]:ch_ival[1],:].reshape(-1)\n",
    "    stimulus_type = stimulus_type.reshape(-1)[:-n_s].reshape(n_ch,-1)[ch_ival[0]:ch_ival[1],:].reshape(-1)\n",
    "    # Erase last 768 samples in order to reshape to (n_ch,-1,64 )\n",
    "    data = data.reshape(-1)[:-n_s*64].reshape(n_ch,-1,64)\n",
    "    data = data[ch_ival[0]:ch_ival[1],:,:] # Get first 20 characters for training data \n",
    "    data = data.reshape(-1, 64)\n",
    "    fs = 240\n",
    "    stimulus_code = stimulus_code[flashing == 1]\n",
    " \n",
    "    timeaxis = np.linspace(0, data.shape[0] / fs * 1000, data.shape[0], endpoint=False)\n",
    "    dat = Data(data=data, axes=[timeaxis, channels], names=['time', 'channel'], units=['ms', '#'])\n",
    "    dat.fs = fs\n",
    "    \n",
    "#     # preparing the markers\n",
    "    target_mask = np.logical_and((flashing == 1), (stimulus_type == 1)) if len(stimulus_type) > 0 else []\n",
    "    nontarget_mask = np.logical_and((flashing == 1), (stimulus_type == 0)) if len(stimulus_type) > 0 else []\n",
    "    flashing = (flashing == 1)\n",
    "    flashing = [[i, 'flashing'] for i in timeaxis[flashing]]\n",
    "    targets = [[i, 'target'] for i in timeaxis[target_mask]]\n",
    "    nontargets = [[i, 'nontarget'] for i in timeaxis[nontarget_mask]]\n",
    "    dat.stimulus_code = stimulus_code[:]\n",
    "    stim = []\n",
    "    for i,_ in enumerate(flashing):\n",
    "        stim.append([flashing[i][0], STIMULUS_CODE[stimulus_code[i]]])\n",
    "    stimulus_code = stim\n",
    "    markers = flashing[:]\n",
    "    markers.extend(targets)\n",
    "    markers.extend(nontargets)\n",
    "    markers.extend(stimulus_code)\n",
    "    markers.sort()\n",
    "    dat.markers = markers[:]\n",
    "   # dat.sequence_phase = sequence_phase\n",
    "    return dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = '../../BCI_Comp_II_2003/data/'\n",
    "datafiles = [join(path_,f) for f in listdir(path_) if isfile(join(path_, f))]\n",
    "training_set = [f for i,f in enumerate(datafiles) if '12' not in f and 'eloc64' not in f]\n",
    "testing_set = [f for i,f in enumerate(datafiles) if '12' in f and 'eloc64' not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training set\n",
    "train_C = load_bci_data2(training_set,ch_ival=[0,42])\n",
    "test_C = load_bci_data2(testing_set,ch_ival=[0,31], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "labels = 'CATDOGFISHWATERBOWLHATHATGLOVESHOESFISHRAT'\n",
    "train_C.labels = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data_subjs.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([train_A, train_Abis, train_B, train_Bbis, train_C, \n",
    "                 test_A, test_Abis, test_B, test_Bbis, test_C], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
